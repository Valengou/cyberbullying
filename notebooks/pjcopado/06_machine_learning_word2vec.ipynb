{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4856abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ae5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn import set_config; set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f423a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this one can make an analogy in mathematical t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clarification for you and zundark s right i sh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elected or electoral jhk</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is such a fun entry devotchka i once had ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>please relate the ozone hole to increases in c...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  this one can make an analogy in mathematical t...     0.0\n",
       "1  clarification for you and zundark s right i sh...     0.0\n",
       "2                           elected or electoral jhk     0.0\n",
       "3  this is such a fun entry devotchka i once had ...     0.0\n",
       "4  please relate the ozone hole to increases in c...     0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lem = pd.read_csv('../../cyberbullying/data/lem_all_df.csv')\n",
    "df_lem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87f29242",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_used = 1\n",
    "df_sample = df_lem.sample(frac=percentage_used, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415131f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "312f4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(percentage_of_sentences=None):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_sample['text'],df_sample['target'],test_size=0.3,random_state=0)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(X_train))\n",
    "        X_train, y_train = X_train[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(X_test))\n",
    "        X_test, y_test = X_test[:len_test], y_test[:len_test]\n",
    "    \n",
    "    #X_train = [text_to_word_sequence(_) for _ in X_train]\n",
    "    #X_test = [text_to_word_sequence(_) for _ in X_test]\n",
    "    \n",
    "    X_train = X_train.map(text_to_word_sequence)\n",
    "    X_test = X_test.map(text_to_word_sequence)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d75c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of your embedding space = size to represent each word\n",
    "embedding_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d248451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=X_train, vector_size=embedding_size, window=5, min_count=5) # jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18eca2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cunt', 0.8893594741821289),\n",
       " ('idiot', 0.8617633581161499),\n",
       " ('fucking', 0.8492628335952759),\n",
       " ('prick', 0.825516402721405),\n",
       " ('disgrace', 0.8252130150794983),\n",
       " ('retard', 0.8221591711044312),\n",
       " ('moron', 0.819510817527771),\n",
       " ('loser', 0.8143708109855652),\n",
       " ('jerk', 0.8118101954460144),\n",
       " ('ass', 0.8070364594459534)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_1 = word2vec.wv.most_similar('asshole')\n",
    "comm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7ac1aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2vec.wv.key_to_index) # jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1e17e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "178b5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the training and test embedded sentences\n",
    "maxlen = 100\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c57814b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159196, 100, 32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ffd79ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68227, 100, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6faa8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ME\n",
    "for X in [X_train_pad, X_test_pad]:\n",
    "    assert type(X) == np.ndarray\n",
    "    assert X.shape[-1] == word2vec.wv.vector_size\n",
    "\n",
    "\n",
    "assert X_train_pad.shape[0] == len(X_train)\n",
    "assert X_test_pad.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1867e12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (maxlen,embedding_size)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341ce34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542de59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7888a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d3171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a061605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c6de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1632f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(df_sample['text'], df_sample['target'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d5acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = pd.DataFrame(data=X_train, columns=['text'])\n",
    "#X_test = pd.DataFrame(data=X_test, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c617186",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d562ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(cv_results):\n",
    "    score_results = {'accuracy': None, 'precision': None, 'recall': None, 'f1': None, 'time': None}\n",
    "    for key in score_results.keys():\n",
    "        if key == 'time':\n",
    "            score_results[key] = round(cv_results['fit_time'].mean() + cv_results['score_time'].mean(), 1)\n",
    "        else:\n",
    "            score_results[key] = round(cv_results[f'test_{key}'].mean(), 4)\n",
    "    return score_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c1f4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mx_all(y_test, y_pred):\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    TN = cm[0,0]\n",
    "    TP = cm[1,1]\n",
    "    FN = cm[1,0]\n",
    "    FP = cm[0,1]\n",
    "    \n",
    "    recall = np.round_(TP/(TP+FN),3)\n",
    "    precision = np.round_(TP/(TP+FP),3)\n",
    "    accuracy = np.round_((TP+TN)/(TP+TN+FP+FN),3)\n",
    "    F1= np.round((2*precision*recall)/(precision+recall), 3)\n",
    "    \n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1-score: {F1}\")\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[0,1])\n",
    "    disp.plot();\n",
    "    \n",
    "    return recall, precision, accuracy, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511d97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(vectorizer_list, learner_list, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    def list_params(new_class):\n",
    "        name = new_class.__class__.__name__\n",
    "        default_class = eval(name + '()').get_params()\n",
    "        new_class = new_class.get_params()\n",
    "\n",
    "        new_dict = {}\n",
    "\n",
    "        for key in new_class.keys():\n",
    "            if new_class[key] != default_class[key]:\n",
    "                new_dict[key] = new_class[key]\n",
    "        return new_dict\n",
    "    \n",
    "    \n",
    "    # Get length of Training Data:\n",
    "    size = len(y_train)\n",
    "    \n",
    "    results = {}\n",
    "    final_results = []\n",
    "    \n",
    "    for vectorizer in vectorizer_list:\n",
    "        \n",
    "        X_train_vec = vectorizer.fit_transform(X_train['text'])\n",
    "        X_test_vec = vectorizer.transform(X_test['text'])\n",
    "        \n",
    "    \n",
    "        for learner in learner_list:\n",
    "\n",
    "            # Store the vectorizer name and params:\n",
    "            results['Vectorizer'] = vectorizer.__class__.__name__\n",
    "            results['Vectorizer Params'] = list_params(vectorizer)\n",
    "\n",
    "            # Store the learner name and params:\n",
    "            results['Algorithm'] = learner.__class__.__name__\n",
    "            results['Algorithm Params'] = list_params(learner)\n",
    "\n",
    "            # Fit the learner:\n",
    "            start = time() # Get start time\n",
    "            learner = learner.fit(X_train_vec, y_train)\n",
    "            end = time() # Get end time\n",
    "\n",
    "            # Store the training time\n",
    "            results['Training Time'] = round(end - start, 2)\n",
    "\n",
    "            start = time() # Get start time\n",
    "            predictions_test = learner.predict(X_test_vec)\n",
    "            predictions_train = learner.predict(X_train_vec)\n",
    "            end = time() # Get end time\n",
    "\n",
    "            # Store the prediction time\n",
    "            results['Prediction Time'] = round(end - start, 2)\n",
    "\n",
    "            # Compute the F1 Score on Test Set\n",
    "            results['f1'] = round(f1_score(y_test, predictions_test), 4)\n",
    "\n",
    "            # Compute the Recall on Test Set\n",
    "            results['recall'] = round(recall_score(y_test, predictions_test), 4)\n",
    "            \n",
    "            # Compute the Precision on Test Set\n",
    "            results['precision'] = round(precision_score(y_test, predictions_test), 4)\n",
    "            \n",
    "            # Compute the Accuracy on Test Set\n",
    "            results['accuracy'] = round(accuracy_score(y_test, predictions_test), 4)\n",
    "\n",
    "            final_results.append(results.copy())\n",
    "            \n",
    "    # Return a dataframe of the results\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70b46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c205511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95957772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of vectorizers\n",
    "vectorizers = [TfidfVectorizer(),\n",
    "]\n",
    "\n",
    "# make a list of models\n",
    "models = [LinearSVC(C=1.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13a3b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "457ff5ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19676/3731752708.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Penalty term must be positive; got (C=%r)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "model.fit(X_train_embed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b69d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
